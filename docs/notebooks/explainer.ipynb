{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from src.vae_architectures.lstm import LSTMVariationalAutoEncoder\n",
    "from src.vae_architectures.signal_cnn import SignalCNNVariationalAutoEncoder\n",
    "from src.vae_architectures.graph_cnn import GraphVariationalAutoEncoder\n",
    "from src.dataset import ExerciseDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.constants import (\n",
    "    HIDDEN_SIZE,\n",
    "    LATENT_SIZE,\n",
    "    NUM_JOINTS,\n",
    "    NUM_LAYERS,\n",
    "    SEQUENCE_LENGTH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_lstm_autoencoder = LSTMVariationalAutoEncoder(\n",
    "    SEQUENCE_LENGTH, NUM_JOINTS * 3, HIDDEN_SIZE, LATENT_SIZE, NUM_LAYERS\n",
    ")\n",
    "dct_lstm_autoencoder.load_state_dict(\n",
    "    torch.load(\"../../models/squat/dct_lstm.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "dct_cnn_autoencoder = SignalCNNVariationalAutoEncoder(\n",
    "    SEQUENCE_LENGTH, NUM_JOINTS * 3, HIDDEN_SIZE, LATENT_SIZE\n",
    ")\n",
    "dct_cnn_autoencoder.load_state_dict(\n",
    "    torch.load(\"../../models/squat/dct_cnn.pt\", map_location=torch.device(\"cpu\"))\n",
    ")\n",
    "\n",
    "dct_graph_autoencoder = GraphVariationalAutoEncoder(\n",
    "    SEQUENCE_LENGTH, NUM_JOINTS * 3, HIDDEN_SIZE, LATENT_SIZE\n",
    ")\n",
    "dct_graph_autoencoder.load_state_dict(\n",
    "    torch.load(\"../../models/squat/dct_graph.pt\", map_location=torch.device(\"cpu\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_cnn = sum(p.numel() for p in dct_cnn_autoencoder.parameters())\n",
    "total_params_graph = sum(p.numel() for p in dct_graph_autoencoder.parameters())\n",
    "\n",
    "print(\"Liczba parametrów modelu dct_cnn_autoencoder:\", total_params_cnn)\n",
    "print(\"Liczba parametrów modelu dct_graph_autoencoder:\", total_params_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squat_dct_df = pd.read_csv(\"../../data/train/squat/dct.csv\")\n",
    "squat_dct_dataset = ExerciseDataset(squat_dct_df, representation=\"dct\")\n",
    "\n",
    "squat_dct_test_df = pd.read_csv(\"../../data/test/squat/dct.csv\")\n",
    "squat_dct_dataset_test = ExerciseDataset(squat_dct_test_df, representation=\"dct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of the embedded instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = torch.stack([rep for rep in squat_dct_dataset.data])\n",
    "y = np.array([1 if label == 0 else 0 for label in squat_dct_dataset.labels_encoded])\n",
    "\n",
    "X_test = torch.stack([rep for rep in squat_dct_dataset_test.data])\n",
    "y_test = np.array(\n",
    "    [1 if label == 0 else 0 for label in squat_dct_dataset_test.labels_encoded]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_squat_dct_dl = DataLoader(\n",
    "    squat_dct_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"lstm\": dct_lstm_autoencoder,\n",
    "    \"cnn\": dct_cnn_autoencoder,\n",
    "    \"graph\": dct_graph_autoencoder,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "fig, axes = plt.subplots(ncols=3, figsize=(14, 4))\n",
    "\n",
    "for (model_name, model), axis in zip(models.items(), axes):\n",
    "    X_embedded = model.encoder(X)[0].detach().numpy()\n",
    "    X_test_embedded = model.encoder(X_test)[0].detach().numpy()\n",
    "\n",
    "    latent_space = tsne.fit_transform(np.concatenate([X_embedded, X_test_embedded]))\n",
    "    all_y = np.concatenate([y, y_test])\n",
    "    axis.scatter(\n",
    "        latent_space[all_y == 1][:, 0], latent_space[all_y == 1][:, 1], c=\"green\"\n",
    "    )\n",
    "    axis.scatter(\n",
    "        latent_space[all_y == 0][:, 0], latent_space[all_y == 0][:, 1], c=\"red\"\n",
    "    )\n",
    "\n",
    "    axis.legend([\"Correct\", \"Incorrect\"])\n",
    "    axis.set_title(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    f1 = []\n",
    "    for _ in range(50):\n",
    "        X_embedded = model.encoder(X)[0].detach().numpy()\n",
    "        X_test_embedded = model.encoder(X_test)[0].detach().numpy()\n",
    "\n",
    "        clf = DecisionTreeClassifier().fit(X_embedded, y)\n",
    "        y_pred = clf.predict(X_test_embedded)\n",
    "        f1.append(f1_score(y_test, y_pred))\n",
    "    print(f\"{model_name} mean f1-score: {np.mean(f1)}, std: {np.std(f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../../models/clf.pkl\", \"rb\") as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate CFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.explainer import Explainer\n",
    "from src.utils.data import get_random_sample\n",
    "\n",
    "wrong_sample, sample_length, label = get_random_sample(train_squat_dct_dl, 3)\n",
    "explainer = Explainer(dct_lstm_autoencoder, clf, train_squat_dct_dl, \"squat\")\n",
    "latent_query, cf_sample, cf_sample_decoded = explainer.generate_cf(wrong_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "latent_space = tsne.fit_transform(\n",
    "    np.concatenate([X_embedded, X_test_embedded, cf_sample, latent_query])\n",
    ")\n",
    "all_y = np.concatenate([y, y_test, [4], [5]])\n",
    "plt.scatter(latent_space[all_y == 1][:, 0], latent_space[all_y == 1][:, 1], c=\"green\")\n",
    "plt.scatter(latent_space[all_y == 0][:, 0], latent_space[all_y == 0][:, 1], c=\"red\")\n",
    "plt.scatter(latent_space[all_y == 4][:, 0], latent_space[all_y == 4][:, 1], c=\"yellow\")\n",
    "plt.scatter(latent_space[all_y == 5][:, 0], latent_space[all_y == 5][:, 1], c=\"black\")\n",
    "\n",
    "plt.legend([\"Correct\", \"Incorrect\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "sample_id = 3\n",
    "\n",
    "query_instance = X_test_embedded[sample_id]\n",
    "\n",
    "closest_correct_instances = cdist(\n",
    "    np.expand_dims(query_instance, 0), X_test_embedded\n",
    ").squeeze()\n",
    "mask = np.where(y_test == 1)[0]\n",
    "\n",
    "mask_argmin = closest_correct_instances[mask].argmin()\n",
    "cf_id = mask[mask_argmin]\n",
    "cf_instance = X_test_embedded[cf_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode latent_space to DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_dct = dct_lstm_autoencoder.decoder(torch.tensor(cf_instance).unsqueeze(0))\n",
    "original_dct = dct_lstm_autoencoder.decoder(torch.tensor(wrong_sample).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode DCT to pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualization import get_3D_animation\n",
    "from src.utils.data import decode_dct\n",
    "\n",
    "\n",
    "cf_sample = decode_dct(\n",
    "    cf_dct.detach().numpy().squeeze(), squat_dct_dataset.lengths[sample_id]\n",
    ")\n",
    "original_sample = decode_dct(\n",
    "    original_dct.detach().numpy().squeeze(), squat_dct_dataset.lengths[sample_id]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sample_anim = get_3D_animation(original_sample, color=\"red\")\n",
    "cf_sample_anim = get_3D_animation(cf_sample, color=\"green\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "html_code = f\"\"\"\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"original_sample.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "<video width=\"400\" height=\"300\" controls>\n",
    "  <source src=\"cf_sample.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(html_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.visualization import get_3D_animation_comparison\n",
    "\n",
    "comparison_anim = get_3D_animation_comparison(original_sample, cf_sample)\n",
    "\n",
    "HTML(comparison_anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
